{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and preprocessing\n",
    "\n",
    "Luckily, thanks to a preprocessed dataset and handy packages, there is almost nothing to do. The result is a sparse matrix `train_X` of dimension `n_docs` by `n_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this once when you first get started\n",
    "#! gunzip associated-press.tar.gz && tar -xvf associated-press.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = []\n",
    "with open(\"associated-press/train.dat\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        train_docs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=None)\n",
    "train_X = vectorizer.fit_transform(train_docs)\n",
    "train_X = train_X.toarray() # sparse matrix is overkill for this dataset\n",
    "\n",
    "# you may want to debug using just a fraction of the data in order to speed up the development cycle.\n",
    "frac_data = 1.\n",
    "n_docs, n_tokens = train_X.shape\n",
    "train_X = train_X[:int(frac_data*n_docs),:]\n",
    "\n",
    "n_docs, n_tokens = train_X.shape\n",
    "n_topics = ## (SOLVE) set the number of topics you wish to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(n_topics):\n",
    "    ## (SOLVE) Initialize matrices U, V, and Q.\n",
    "    ## HINT: Don't forget that these matrices must satisfy constraints.\n",
    "    \n",
    "    pass\n",
    "#    return U,V,Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define functions to do the E-step and the M-step as well as compute the current value of the log-likelihood as well as the lower bound $\\mathcal L(X; U,V,Q) := \\sum_{ij} X_{ij}\\sum_{z=1}^K Q_{zij}[\\log U_{iz} + \\log V_{jz} - \\log Q_{zij}]$, which is our actual optimization objective. Then, we run the EM algorithm for some steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expectation_step(U, V):\n",
    "    ## (SOLVE) perform the expectation step here\n",
    "    ## HINT: double check that the constraints are satisfied\n",
    "    pass\n",
    "    #     return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(U, V, Q, X, eps=1e-2):\n",
    "    ## (SOLVE) perform the maximization step here\n",
    "    ## HINT: double check that the constraints are satisfied\n",
    "    ## HINT: be careful not to divide by zero. Use the eps variable if necessary.\n",
    "    pass\n",
    "#     return Uprime, Vprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound(U, V, Q, X):    \n",
    "    ## (SOLVE) compute the lower bound here\n",
    "\n",
    "def compute_loglikelihood(U, V, Q, X, eps=1e-4): \n",
    "    ## (SOLVE) compute the loglikelihood here.\n",
    "    ## HINT use eps if necessary to prevent errors with np.log(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,V,Q = initialize_model(n_topics)\n",
    "n_itrs = 25 # number of iterations \n",
    "lowerbounds = np.zeros(n_itrs)\n",
    "loglikelihoods = np.zeros(n_itrs)\n",
    "for i in range(n_itrs):\n",
    "    print(\"EM iteration {}\".format(i))\n",
    "    Q = expectation_step(U, V)\n",
    "    \n",
    "    ## (SOLVE) check if the lower bound is maximized in the E-step.\n",
    "    \n",
    "    U, V = maximization_step(U, V, Q, train_X)\n",
    "    lowerbounds[i] = compute_lower_bound(U,V,Q,train_X)\n",
    "    loglikelihoods[i] = compute_loglikelihood(U,V,Q,train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (SOLVE) plot the lowerbounds and the likelihoods. Verify that they are increasing upon each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "For each topic, we print the top 5 associated words i.e. the words for which $V[w,topic]$ is maximized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 5\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "## (SOLVE) do what we claim in the text above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
